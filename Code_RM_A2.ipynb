{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPVujlWmUdRx5MOI78XxY7j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajaniBoddupally/Assignment2-RM/blob/main/Code_RM_A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers -q\n",
        "!pip install -U scikit-learn -q\n",
        "!pip install wordcloud -q\n",
        "# Import all required libraries for data processing, visualization, modeling, and evaluation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# Set the device to GPU if available, else fallback to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "id": "SJNKSOKIv6TI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EarFdW8L9knD"
      },
      "outputs": [],
      "source": [
        "# Define the URL of the CSV file from the GitHub repository\n",
        "Emotefile = 'https://raw.githubusercontent.com/RajaniBoddupally/Assignment2-RM/refs/heads/main/Emotion_classify_Data.csv'\n",
        "\n",
        "# Read the CSV file from the specified URL into a DataFrame\n",
        "Emotefile_Data = pd.read_csv(Emotefile)\n",
        "\n",
        "# Print the first 5 rows of the DataFrame\n",
        "print(Emotefile_Data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all column names in the DataFrame to lowercase\n",
        "Emotefile_Data.columns = [col.lower() for col in Emotefile_Data.columns]\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print('\\n Dataset Info:\\n')\n",
        "print(Emotefile_Data.info())\n",
        "\n",
        "# Show descriptive statistics for numerical columns\n",
        "print('\\n Descriptive Statistics:\\n', Emotefile_Data.describe(), '\\n\\n')\n",
        "\n",
        "# Check and print the number of duplicate rows in the dataset\n",
        "print('\\nNumber of Duplicated Rows:', Emotefile_Data.duplicated().sum(), '\\n\\n')\n"
      ],
      "metadata": {
        "id": "QkMMnIu_v_vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for null (missing) values in each column\n",
        "print(\"Null values in each column:\\n\")\n",
        "print(Emotefile_Data.isnull().sum())\n",
        "\n",
        "# Display rows with missing values if any are found\n",
        "if Emotefile_Data.isnull().any().any():\n",
        "    print(\"\\nRows with null values:\\n\")\n",
        "    display(Emotefile_Data[Emotefile_Data.isnull().any(axis=1)])\n",
        "else:\n",
        "    print(\"\\nâœ… No missing values found in the dataset.\")\n"
      ],
      "metadata": {
        "id": "uitx7k8CwLHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the distribution of emotion labels without future warning\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(data=Emotefile_Data, x='emotion', hue='emotion', palette='Set2', legend=False)\n",
        "plt.title('Distribution of Emotions')\n",
        "plt.xlabel('Emotion')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rB-yNkD1wR9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the count of each emotion category\n",
        "emotion_counts = Emotefile_Data['emotion'].value_counts()\n",
        "emotion_labels = emotion_counts.index\n",
        "emotion_sizes = emotion_counts.values\n",
        "\n",
        "# Create a pie chart to show the percentage distribution of emotions\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.pie(emotion_sizes,\n",
        "        labels=emotion_labels,\n",
        "        autopct='%1.1f%%',\n",
        "        startangle=140,\n",
        "        colors=plt.cm.Set3.colors)\n",
        "\n",
        "# Set title and making sure that the pie is a perfect circle\n",
        "plt.title('Percentage Distribution of Emotions')\n",
        "plt.axis('equal')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_SvKmqJNwkL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the 'emotion' column to categorical type and encode it as numeric labels\n",
        "Emotefile_Data['emotion'] = Emotefile_Data['emotion'].astype('category')\n",
        "Emotefile_Data['label'] = Emotefile_Data['emotion'].cat.codes\n",
        "\n",
        "# Extract the list of original emotion categories in order of their assigned codes\n",
        "Emo_Lable_nms = Emotefile_Data['emotion'].cat.categories.tolist()\n",
        "\n",
        "# Display the mapping between numeric labels and emotion categories\n",
        "print(\"Label mapping:\", dict(enumerate(Emo_Lable_nms)))\n"
      ],
      "metadata": {
        "id": "6kCwuP1Ywwq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new column that stores the length of each comment\n",
        "Emotefile_Data['EM_Comment_Len'] = Emotefile_Data['comment'].apply(len)\n",
        "\n",
        "# Display summary statistics of the comment lengths\n",
        "print(\"\\nText Length Stats:\\n\", Emotefile_Data['EM_Comment_Len'].describe())\n",
        "\n",
        "# Plot a histogram showing the distribution of comment lengths\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(Emotefile_Data['EM_Comment_Len'], bins=30, kde=True)\n",
        "plt.title(\"Distribution of Comment Lengths\")\n",
        "plt.xlabel(\"Number of Characters\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TeTSZc1kwzrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a boxplot to compare comment lengths across different emotion categories\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=Emotefile_Data, x='emotion', y='EM_Comment_Len', palette='Pastel2')\n",
        "plt.title(\"Comment Length by Emotion\")\n",
        "plt.xlabel(\"Emotion\")\n",
        "plt.ylabel(\"Text Length\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DJpezV3kxDzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate word clouds for each unique emotion category\n",
        "emotions = Emotefile_Data['emotion'].unique()\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "for i, emotion in enumerate(emotions):\n",
        "    # Combine all comments for the current emotion into a single string\n",
        "    text = \" \".join(Emotefile_Data[Emotefile_Data['emotion'] == emotion]['comment'])\n",
        "\n",
        "    # Generate a word cloud from the combined text\n",
        "    wordcloud = WordCloud(width=600, height=400, background_color='white', colormap='viridis').generate(text)\n",
        "\n",
        "    # Plot the word cloud\n",
        "    plt.subplot(1, len(emotions), i + 1)\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"{emotion.capitalize()}\")\n",
        "\n",
        "# Adjust layout and display all word clouds\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ggCn0SlGxJLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "# Initialize the WordNet lemmatizer and define English stopwords\n",
        "lem_emo_data = WordNetLemmatizer()\n",
        "Emo_Stop_Words = set(stopwords.words('english'))\n",
        "\n",
        "# Define a function to clean and preprocess emotion comment text\n",
        "def Emote_Change_PP(Emo_Comment):\n",
        "    \"\"\"\n",
        "    Clean and preprocess a given emotion comment.\n",
        "\n",
        "    This function performs the following steps:\n",
        "    1. Converts text to lowercase\n",
        "    2. Removes punctuation and numeric characters\n",
        "    3. Splits the text into tokens (words)\n",
        "    4. Removes English stopwords\n",
        "    5. Applies lemmatization to each word\n",
        "\n",
        "    Parameters:\n",
        "    text (str): The raw comment text to be cleaned\n",
        "\n",
        "    Returns:\n",
        "    str: The cleaned and lemmatized text\n",
        "    \"\"\"\n",
        "    Emo_lower = Emo_Comment.lower()\n",
        "    Emo_Clean = re.sub(r'[^a-z\\s]', '', Emo_lower)\n",
        "    Emo_Token = Emo_Clean.split()\n",
        "    Emo_lem_token = [lem_emo_data.lemmatize(word) for word in Emo_Token if word not in Emo_Stop_Words]\n",
        "    return ' '.join(Emo_lem_token)\n",
        "\n",
        "\n",
        "# Apply the preprocessing function to the 'comment' column\n",
        "Emotefile_Data['Emote_cleaned'] = Emotefile_Data['comment'].apply(Emote_Change_PP)\n",
        "# Display original and cleaned comment text for comparison\n",
        "Emotefile_Data[['comment', 'Emote_cleaned']].head()\n"
      ],
      "metadata": {
        "id": "usjV6L3UxcXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first 5 rows of the dataset\n",
        "Emotefile_Data.head()\n"
      ],
      "metadata": {
        "id": "SkT8FN9Mx4XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets (80/20 split) with stratified sampling\n",
        "Emo_Comment_Train, Emo_Comment_Test, Emo_Lbl_Train, Emo_Lbl_Test = train_test_split(\n",
        "    Emotefile_Data['Emote_cleaned'],\n",
        "    Emotefile_Data['label'],\n",
        "    test_size=0.2,\n",
        "    random_state=123456,\n",
        "    stratify=Emotefile_Data['label']\n",
        ")\n",
        "\n",
        "# Convert target label variables to categorical type\n",
        "Emo_Lbl_Train = Emo_Lbl_Train.astype('category')\n",
        "Emo_Lbl_Test = Emo_Lbl_Test.astype('category')\n"
      ],
      "metadata": {
        "id": "UpJUVAFEBvo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained BERT tokenizer\n",
        "Emo_tokenize = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenize and encode the training and testing comment text\n",
        "Emo_encode_Train = Emo_tokenize(Emo_Comment_Train.tolist(), truncation=True, padding=True, return_tensors='pt')\n",
        "Emo_encode_Test = Emo_tokenize(Emo_Comment_Test.tolist(), truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "# Convert training and testing labels to PyTorch tensors\n",
        "Emo_Trn_lbls = torch.tensor(Emo_Lbl_Train.cat.codes.values)\n",
        "Emo_Test_lbls = torch.tensor(Emo_Lbl_Test.cat.codes.values)\n",
        "\n",
        "# Create TensorDatasets for training and testing\n",
        "Emo_Train_DS = TensorDataset(Emo_encode_Train['input_ids'], Emo_encode_Train['attention_mask'], Emo_Trn_lbls)\n",
        "Emo_Test_DS = TensorDataset(Emo_encode_Test['input_ids'], Emo_encode_Test['attention_mask'], Emo_Test_lbls)\n"
      ],
      "metadata": {
        "id": "1A8NBpoqCKem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained BERT model for sequence classification\n",
        "Emo_Model_Bert = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", num_labels=len(Emo_Lable_nms)\n",
        ").to(device)\n",
        "\n",
        "# Define the optimizer using AdamW\n",
        "Emo_Optz = AdamW(Emo_Model_Bert.parameters(), lr=1e-5)\n",
        "\n",
        "# Define the loss function for multi-class classification\n",
        "Emo_Loss_Fn = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "iF9wskBGCaEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders for training and testing\n",
        "Emo_Train_Ldr = DataLoader(Emo_Train_DS, batch_size=8, shuffle=True)\n",
        "Emo_Test_Ldr = DataLoader(Emo_Test_DS, batch_size=8)\n",
        "\n",
        "# Initialize lists to store accuracy and loss values across epochs\n",
        "Emo_Train_Acc = []\n",
        "Emo_Test_Acc = []\n",
        "Emo_epoch_loss = []\n",
        "\n",
        "# Loop through multiple training epochs\n",
        "for epoch in range(5):  # Adjust the number of epochs as needed\n",
        "    Emo_Model_Bert.train()  # Set model to training mode\n",
        "    Ep_total_loss = 0\n",
        "    Emo_Lbl_crct = 0\n",
        "    Emo_all_ttl = 0\n",
        "\n",
        "    # Iterate through training batches\n",
        "    for batch in tqdm(Emo_Train_Ldr, desc=f\"Epoch {epoch+1} - Training\"):\n",
        "        Emo_Ip_Ids, Emo_At_Masks, Emo_ep_lbls = [b.to(device) for b in batch]\n",
        "        Emo_ep_lbls = Emo_ep_lbls.long()\n",
        "\n",
        "        # Forward pass and loss computation\n",
        "        Emo_Optz.zero_grad()\n",
        "        Emo_ops = Emo_Model_Bert(input_ids=Emo_Ip_Ids, attention_mask=Emo_At_Masks)\n",
        "        Emo_ep_loss = Emo_Loss_Fn(Emo_ops.logits, Emo_ep_lbls)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        Emo_ep_loss.backward()\n",
        "        Emo_Optz.step()\n",
        "\n",
        "        # Accumulate loss and correct predictions\n",
        "        Ep_total_loss += Emo_ep_loss.item()\n",
        "        Emo_bert_pred = torch.argmax(Emo_ops.logits, dim=1)\n",
        "        Emo_Lbl_crct += (Emo_bert_pred == Emo_ep_lbls).sum().item()\n",
        "        Emo_all_ttl += Emo_ep_lbls.size(0)\n",
        "\n",
        "    # Calculate and store training accuracy and loss for this epoch\n",
        "    Eval_Trn_acc = Emo_Lbl_crct / Emo_all_ttl\n",
        "    Emo_Train_Acc.append(Eval_Trn_acc)\n",
        "    Emo_epoch_loss.append(Ep_total_loss / len(Emo_Train_Ldr))\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    Emo_Model_Bert.eval()\n",
        "    Emo_Test_Crct = 0\n",
        "    Emo_Eval_ttl = 0\n",
        "\n",
        "    # Evaluate model on test data without updating gradients\n",
        "    with torch.no_grad():\n",
        "        for batch in Emo_Test_Ldr:\n",
        "            Eval_ips, Eval_att_masks, Eval_lbls = [b.to(device) for b in batch]\n",
        "            Eval_lbls = Eval_lbls.long()\n",
        "            totch_op = Emo_Model_Bert(input_ids=Eval_ips, attention_mask=Eval_att_masks)\n",
        "            Eval_preds = torch.argmax(totch_op.logits, dim=1)\n",
        "            Emo_Test_Crct += (Eval_preds == Eval_lbls).sum().item()\n",
        "            Emo_Eval_ttl += Eval_lbls.size(0)\n",
        "\n",
        "    # Calculate and store test accuracy for this epoch\n",
        "    Eval_Test_acc = Emo_Test_Crct / Emo_Eval_ttl\n",
        "    Emo_Test_Acc.append(Eval_Test_acc)\n",
        "\n",
        "    # Display training and test accuracy along with average training loss\n",
        "    print(f\"Epoch {epoch+1}: Training Accuracy = {Eval_Trn_acc:.4f} \\n , Testing Accuracy = {Eval_Test_acc:.4f}, Loss = {Ep_total_loss / len(Emo_Train_Ldr):.4f}\")\n"
      ],
      "metadata": {
        "id": "EAVNSwh3CtzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and testing accuracy over epochs\n",
        "Num_epochs = list(range(1, len(Emo_Train_Acc) + 1))\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(Num_epochs, Emo_Train_Acc, marker='o', label='Training Accuracy')\n",
        "plt.plot(Num_epochs, Emo_Test_Acc, marker='o', label='Testing Accuracy')\n",
        "plt.title(\"Training vs Testing Accuracy Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xticks(Num_epochs)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ayAWwb-ZHJqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "Emo_Model_Bert.eval()\n",
        "\n",
        "# Create a DataLoader for the test dataset\n",
        "Emo_Test_Ldr = DataLoader(Emo_Test_DS, batch_size=8)\n",
        "\n",
        "# Initialize lists to store predictions and true labels\n",
        "Emo_Berts_Preds, Emo_org_lbls = [], []\n",
        "\n",
        "# Perform inference on test data without computing gradients\n",
        "with torch.no_grad():\n",
        "    for batch in Emo_Test_Ldr:\n",
        "        Emo_test_ips, Emo_test_Att, EMo_test_lbls = [b.to(device) for b in batch]\n",
        "        Emo_test_op = Emo_Model_Bert(input_ids=Emo_test_ips, attention_mask=Emo_test_Att)\n",
        "        Emo_test_pred = torch.argmax(Emo_test_op.logits, dim=1)\n",
        "        Emo_Berts_Preds.extend(Emo_test_pred.cpu().numpy())\n",
        "        Emo_org_lbls.extend(EMo_test_lbls.cpu().numpy())\n",
        "\n",
        "# Compute and display evaluation metrics\n",
        "EVal_Test_acc_all = accuracy_score(Emo_org_lbls, Emo_Berts_Preds)\n",
        "print(f\"Test Accuracy on Unseen Emotion Data: {EVal_Test_acc_all:.2f}\")\n",
        "print(\"\\nClassification Report on Unseen Emotion Data:\\n\", classification_report(Emo_org_lbls, Emo_Berts_Preds, target_names=Emo_Lable_nms))\n"
      ],
      "metadata": {
        "id": "HeBDi9QJHZxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the confusion matrix for the test predictions\n",
        "Emo_Eval_Confmat = confusion_matrix(Emo_org_lbls, Emo_Berts_Preds)\n",
        "EMo_Cm_Disp = ConfusionMatrixDisplay(confusion_matrix=Emo_Eval_Confmat, display_labels=Emo_Lable_nms)\n",
        "EMo_Cm_Disp.plot(cmap = 'Blues')\n",
        "plt.title(\"Confusion Matrix for Emotion Classification\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LEdmZOX7IPlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare predicted and actual emotion labels for 5 random test samples\n",
        "Emo_Test_Samples = np.random.choice(len(Emo_Comment_Test), 5, replace=False)\n",
        "\n",
        "for Ets in Emo_Test_Samples:\n",
        "    Test_Sample_Emo = Emo_Comment_Test.iloc[Ets]\n",
        "    Emo_orgl_Label = Emo_Lable_nms[Emo_Lbl_Test.iloc[Ets]]\n",
        "\n",
        "    # Tokenize the sample and send it to the model\n",
        "    Ips_token = Emo_tokenize(Test_Sample_Emo, return_tensors='pt', truncation=True, padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        Emo_Op_TOken = Emo_Model_Bert(**Ips_token)\n",
        "    Emo_Pred_Samp_Label = Emo_Lable_nms[torch.argmax(Emo_Op_TOken.logits, dim=1).item()]\n",
        "\n",
        "    # Display text, actual label, and predicted label\n",
        "    print(\"\\n Sample Comment from Test Dataset:\", Test_Sample_Emo)\n",
        "    print(\"Actual Emotion   :\", Emo_orgl_Label)\n",
        "    print(\"Predicted Emotion:\", Emo_Pred_Samp_Label)\n"
      ],
      "metadata": {
        "id": "xfT6-Tv-IlcI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}